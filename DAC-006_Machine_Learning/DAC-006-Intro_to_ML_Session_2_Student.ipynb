{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression Tree (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief history on CART and Decision Trees. Proposed by Leo Breiman in the 80s (Breiman et al. 1984, Classification and Regression Tree), was the 2nd generation of Decision Tree Algorithms, succeeding algorithms such as AID (1963, Morgan & Sonquist), THAID (1972, Messenger & Mandell), CHAID (1980, Kass). The most recent developments (4th gen.) for DTAs happened in the early 21st century with GUIDE (Loh et al., 2015), BART (Chipman, 2010) and Random Forest (Breiman, 2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Index & Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Gini Index:\n",
    "$$\n",
    "G(t) = \\sum_{i=1}^{c}p(i|t).(1-p(i|t)) = 1 - \\sum_{i=1}^{c}p(i|t)^2\n",
    "$$\n",
    "\n",
    "Formula for Information Gain:\n",
    "$$\n",
    "IG = G(P) - \\frac{N_L}{N}\\times G(L) - \\frac{N_R}{N}\\times G(R)\n",
    "$$\n",
    "where $G(P)$ is the Gini Index of the parent node; $G(L)/G(R)$ are the Gini Index of the respective children nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data into a dataframe\n",
    "# df = pd.read_csv('datasets/titanic.csv')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are what the column names represent:\n",
    "* PassengerId - unique identifier\n",
    "* Survived - 0 = No, 1 = Yes\n",
    "* Pclass - 1 = 1st class, 2 = 2nd class, 3 = 3rd class\n",
    "* Name \n",
    "* Sex\n",
    "* Age\n",
    "* SibSp - # of siblings / spouses aboard the Titanic\n",
    "* Parch - # of parents / children aboard the Titanic\t\n",
    "* Ticket\n",
    "* Fare\n",
    "* Cabin\n",
    "* Embarked - C = Cherbourg, Q = Queenstown, S = Southampton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Missing Completly at Random (MCAR)\n",
    "      - Missing Completely at Random (MCAR) is a type of missing data where the probability of a data point being missing is independent of all other variables in the data set, including the missing data itself. In other words, the missing data is completely random. \\\n",
    "      \\\n",
    "      An example of MCAR would be a survey where some respondents accidentally skipped a question. The reason for the missing data is unrelated to any of the other data in the survey, such as the respondent's age, gender, or political affiliation. \\\n",
    "      \\\n",
    "      Another example of MCAR would be a data set of customer purchases where some purchases are missing because the customer's credit card was declined. The reason for the missing data is unrelated to the customer's other purchases, such as the type of products they buy or how much they spend. \\\n",
    "      \\\n",
    "      MCAR is the most desirable type of missing data because it is the least likely to bias the results of statistical analyses.\n",
    "2.   Missing at Random (MAR)\n",
    "      - is a type of missing data where the probability of a data point being missing is independent of the missing data itself, but may depend on other observed variables in the data set. In other words, the missing data is not completely random, but it is possible to predict which data points are likely to be missing based on the other data that is available.\n",
    "3.   Missing Not at Random (MNAR)\n",
    "      - is a type of missing data where the probability of a data point being missing is related to the missing data itself, even after accounting for other observed variables in the data set. In other words, the missing data is not random, and it is not possible to fully predict which data points are likely to be missing based on the other data that is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we deal with missing data?\n",
    "1. Deletion\n",
    "2. Imputation (this can range from simply imputing mean, median or mode values into the dataset to creating machine learning algorithms that can estimate the value of the missing data.)\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/missing_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "# df.fillna({'Age': df['Age'].mean(), 'Embarked': df['Embarked'].mode()[0]}, inplace=True)  # Processing steps\n",
    "# df['Sex'] = df['Sex'].factorize()[0] # label encoding\n",
    "# df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "# # Initialize the OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# # Fit and transform the 'Embarked' column\n",
    "# encoded_embarked = encoder.fit_transform(df[['Embarked']])\n",
    "\n",
    "# # Create a DataFrame with the encoded columns\n",
    "# encoded_embarked_df = pd.DataFrame(\n",
    "#     encoded_embarked, \n",
    "#     columns=encoder.get_feature_names_out(['Embarked']),\n",
    "#     index=df.index\n",
    "# )\n",
    "\n",
    "# # Concatenate the original DataFrame with the encoded columns\n",
    "# df = pd.concat([df, encoded_embarked_df], axis=1)\n",
    "\n",
    "# # Drop the original 'Embarked' column\n",
    "# df.drop('Embarked', axis=1, inplace=True)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df['Survived']\n",
    "# x = df.drop('Survived', axis=1)\n",
    "# depth = 3 # depth of the tree\n",
    "\n",
    "\n",
    "# # perform your train test split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# sk_model = DecisionTreeClassifier(max_depth=depth, \n",
    "#                                   random_state=1)\n",
    "# sk_model.fit(x_train, y_train)\n",
    "# sk_pred = sk_model.predict(x_test)\n",
    "# acc = (y_test == sk_pred).mean()\n",
    "# print('DecisionTreeClassifier: accuracy = {:.3f}'.format(acc))\n",
    "\n",
    "# # plotting \n",
    "# feature_names = x.columns.tolist()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# tree.plot_tree(sk_model,\n",
    "#                feature_names=feature_names)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature in a dataset has differing levels of importance. Information Gain (IG) can be used to determine the importance of features in a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "IG_{Sex} = 0.469 - (0.303\\times467/712 - 0.386\\times245/712) = 0.403 \\\\\n",
    "$$\n",
    "\n",
    "Since pclass is used twice and age is used twice, we take the weighted average of the IGs:\n",
    "\n",
    "$$\n",
    "IG_{pclass[1]} = 0.386 - (0.074\\times130/245 - 0.5\\times115/245) = 0.581 \\\\\n",
    "IG_{pclass[2]} = 0.271 - (0.442\\times91/445 - 0.209\\times354/445) = 0.347 \\\\\n",
    "IG_{pclass} = 0.581 \\times 245/712 + 0.347 \\times 445/712 = 0.417\n",
    "$$\n",
    "\n",
    "Same calculation for age:\n",
    "\n",
    "$$\n",
    "IG_{age[1]} = 0.303 - (0.434\\times22/467 - 0.271\\times445/467) = 0.541 \\\\\n",
    "IG_{age[2]} = 0.074 - (0.5\\times2/130 - 0.061\\times128/130) = 0.126 \\\\\n",
    "IG_{age} =  0.541 \\times 467/712 +  0.126 \\times 130/712 = 0.378\n",
    "$$\n",
    "\n",
    "Then we normalize by taking:\n",
    "\n",
    "$$\n",
    "IG_{Sex} = \\frac{IG_{Sex}}{IG_{Sex}+IG_{pclass}+IG_{age}} \\\\\n",
    "\n",
    "IG_{Sex} = 0.336 \\\\\n",
    "IG_{pclass} = 0.348 \\\\\n",
    "IG_{age} = 0.315 \\\\\n",
    "$$\n",
    "\n",
    "The value generated is the **importance** of each feature. Now let's find the optimal depth of the tree and the important features using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reuse the dataframe from previously\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the Titanic data into features and target class.\n",
    "# y = df['Survived']\n",
    "# x = df.drop('Survived', axis=1)\n",
    "\n",
    "# # Split the data into training, validation and test data.\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "\n",
    "# x_test, x_eval, y_test, y_eval = train_test_split(x_test, y_test, test_size = 0.5)\n",
    "\n",
    "# # Create decision tree models of various depths, \n",
    "# # and measure the accuracy of validation data for each model.\n",
    "# train_acc = []\n",
    "# eval_acc = []\n",
    "# max_depth = 8\n",
    "# for d in range(1, max_depth+1):\n",
    "#     model = DecisionTreeClassifier(max_depth=d)\n",
    "#     model.fit(x_train, y_train)\n",
    "    \n",
    "#     # Measure the accuracy of this model using the training data.\n",
    "#     y_pred = model.predict(x_train)\n",
    "#     train_acc.append((y_pred == y_train).mean())\n",
    "\n",
    "#     # Measure the accuracy of this model using the validation data.\n",
    "#     y_pred = model.predict(x_eval)\n",
    "#     eval_acc.append((y_pred == y_eval).mean())\n",
    "#     print('Depth = {}, train_acc = {:.4f}, eval_acc = {:.4f}'\\\n",
    "#           .format(d, train_acc[-1], eval_acc[-1]))\n",
    "\n",
    "# # Find the optimal depth with the highest accuracy of validation data.\n",
    "# opt_depth = np.argmax(eval_acc) + 1\n",
    "\n",
    "# # Visualize accuracy changes as depth changes.\n",
    "# plt.plot(train_acc, marker='o', label='train')\n",
    "# plt.plot(eval_acc, marker='o', label='evaluation')\n",
    "# plt.legend()\n",
    "# plt.title('Accuracy')\n",
    "# plt.xlabel('tree depth')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xticks(np.arange(max_depth), np.arange(1, max_depth+1))\n",
    "# plt.axvline(x=opt_depth-1, ls='--')\n",
    "# plt.ylim(0.5, 1.0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I set max_step=3 as a constant value for tree visualization.\n",
    "# model = DecisionTreeClassifier(max_depth=3)\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# # Use test data to evaluate final performance.\n",
    "# y_pred = model.predict(x_test)\n",
    "# test_acc = (y_pred == y_test).mean()\n",
    "# print('Optimal depth = {}, test_acc = {:.4f}'.format(opt_depth, test_acc))\n",
    "        \n",
    "# # Visualize the tree\n",
    "# # plt.figure(figsize=(20,10))\n",
    "# feat_names = x.columns.tolist()\n",
    "# plt.figure(figsize=(14,6))\n",
    "# tree.plot_tree(model, feature_names = feat_names, fontsize=10)\n",
    "# plt.show()\n",
    "\n",
    "# # Analyze the importance of features.\n",
    "# feature_importance = model.feature_importances_\n",
    "# n_feature = x_train.shape[1]\n",
    "# idx = np.arange(n_feature)\n",
    "\n",
    "# plt.barh(idx, feature_importance, align='center', color='green')\n",
    "# plt.yticks(idx, feat_names, size=12)\n",
    "# plt.xlabel('importance', size=15)\n",
    "# plt.ylabel('feature', size=15)\n",
    "# plt.show()\n",
    "\n",
    "# print('feature importance = {}'.format(feature_importance.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As a tree gets deeper, error in training data decreases, however error in test data may increase (overfitting).\n",
    "* To prevent this, pruning of branches in the complex tree is done to lower the complexity \n",
    "* In the previous code, we set the max_depth in advance based on repeated testing of different points of max_depth (technique is known as cross-validation) - this is a pre-pruning method to deal with overfitting\n",
    "* Here we learn a post-pruning algorithm called Cost Complexity Pruning (CCP) algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Complexity Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm works like this: \n",
    "\n",
    "1.) Build a tree as deep as possible\n",
    "\n",
    "2.) Calculate a measure called \"cost complexity\" for each node. This measure balances two factors:\n",
    "\n",
    "* The error rate if we keep the subtree below this node\n",
    "* The complexity of the subtree (basically, how many nodes it has)\n",
    "\n",
    "3.) Find the weakest link: the node where removing its subtree would lead to the smallest increase in error relative to the decrease in complexity $(\\alpha)$.\n",
    "\n",
    "4.) Snip off that subtree, replacing it with a leaf node.\n",
    "\n",
    "5.) Repeat steps 3-4, creating a series of progressively simpler trees.\n",
    "\n",
    "6.) Use cross-validation to pick the best pruned tree that balances accuracy and simplicity by creating a list of $\\alpha$ candidates. \n",
    "\n",
    "$$ \n",
    "\\text{Cost Complexity: } C(T) = R(T) + \\alpha|T| \\\\\n",
    "$$\n",
    "where $R(T)$ is the misclassification rate of a tree T, $\\alpha$ the regularization constant, and $|T|$ being the number of leaf nodes in the tree. The objective is to **minimise** $C(T)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reuse the dataframe from previously\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the Titanic data into features and target class.\n",
    "# y = df['Survived']\n",
    "# x = df.drop('Survived', axis=1)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "\n",
    "# # Apply Cost Complexity Pruning (CCP) and get the alpha-star list.\n",
    "# model = DecisionTreeClassifier()\n",
    "# path = model.cost_complexity_pruning_path(x_train, y_train)\n",
    "# ccp_alpha = path.ccp_alphas[:-1]  # exclude the last one.\n",
    "# impurity = path.impurities[:-1]\n",
    "\n",
    "# # Observe impurity changes for alpha changes.\n",
    "# # As alpha increases, the penalty for |T| increases, resulting in\n",
    "# # simple trees and increased impurity (misclassification error).\n",
    "# plt.figure(figsize=(7,4))\n",
    "# plt.plot(ccp_alpha, impurity, marker='o')\n",
    "# plt.xlabel(\"effective alpha\")\n",
    "# plt.ylabel(\"total impurity of leaves\")\n",
    "# plt.title(\"Total Impurity vs effective alpha for training set\")\n",
    "# plt.show()\n",
    "\n",
    "# # C(T) = R(T) + α|T|\n",
    "# # Create trees for each alpha in the alpha-list.\n",
    "# models = []\n",
    "# for i, alpha in enumerate(ccp_alpha):\n",
    "#     model = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "#     model.fit(x_train, y_train)\n",
    "#     models.append(model)\n",
    "#     print('%d) alpha = %.4f done.' % (i, alpha))\n",
    "\n",
    "# # You can see that as alpha increases, \n",
    "# # the number and depth of nodes decrease.\n",
    "# node_counts = [model.tree_.node_count for model in models]\n",
    "# depth = [model.tree_.max_depth for model in models]\n",
    "\n",
    "# fig, ax = plt.subplots(2, 1)\n",
    "# ax[0].plot(ccp_alpha, node_counts, marker=\"o\")\n",
    "# ax[0].set_xlabel(\"alpha\")\n",
    "# ax[0].set_ylabel(\"number of nodes\")\n",
    "# ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "# ax[1].plot(ccp_alpha, depth, marker=\"o\")\n",
    "# ax[1].set_xlabel(\"alpha\")\n",
    "# ax[1].set_ylabel(\"depth of tree\")\n",
    "# ax[1].set_title(\"Depth vs alpha\")\n",
    "# fig.tight_layout()\n",
    "\n",
    "# # Among the candidate trees, find the one with the lowest \n",
    "# # misclassification rate on the test data. It's the same to find \n",
    "# # the tree with the highest score.\n",
    "# # Calculate the score of the tree (model) with alpha applied.\n",
    "# train_score = [model.score(x_train, y_train) for model in models]\n",
    "# test_score = [model.score(x_test, y_test) for model in models]\n",
    "\n",
    "# # Find the alpha that creates the tree with the highest score on \n",
    "# # the test data. This is the optimal alpha, and the tree is optimal.\n",
    "# i_max = np.argmax(test_score)\n",
    "# opt_alpha = ccp_alpha[i_max]\n",
    "# opt_model = models[i_max]\n",
    "\n",
    "# # Observe the change in score for the change in alpha.\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.plot(ccp_alpha, train_score, marker='o', label='train')\n",
    "# plt.plot(ccp_alpha, test_score, marker='o', label='test')\n",
    "# plt.axvline(x=opt_alpha, ls='--', lw=1.0)\n",
    "# plt.legend()\n",
    "# plt.xlabel('alpha')\n",
    "# plt.ylabel('tree score')\n",
    "# plt.show()\n",
    "\n",
    "# # Evaluate the performance of the final tree.\n",
    "# print('Accuracy of test data = %.4f' % opt_model.score(x_test, y_test))\n",
    "# print('Optimal alpha = %.8f' % opt_alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our dataset\n",
    "# iris = load_iris(as_frame=True)\n",
    "# x = iris.data \n",
    "# y = iris.target\n",
    "\n",
    "# # Generate training and test data\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DecisionTreeClassifier\n",
    "# model = DecisionTreeClassifier(max_depth=3)\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# # Estimate the class of validation date.\n",
    "# y_pred = model.predict(x_test)\n",
    "\n",
    "# # Measure the accuracy for validation data\n",
    "# accuracy = (y_test == y_pred).mean()\n",
    "# print('Accuracy of Model = {:.3f}'.format(accuracy))\n",
    "\n",
    "# # Visualize the tree\n",
    "# # plt.figure(figsize=(20,10))\n",
    "# feat_names = x.columns.tolist()\n",
    "# plt.figure(figsize=(14,6))\n",
    "# tree.plot_tree(model, feature_names = feat_names, fontsize=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data and draw the estimated curve.\n",
    "# def plot_prediction(x, y, x_test, y_pred, title):\n",
    "#     plt.figure(figsize=(6,4))\n",
    "#     plt.scatter(x, y, c='blue', s=20, alpha=0.5, label='train data')\n",
    "#     plt.plot(x_test, y_pred, c='red', lw=2.0, label='prediction')\n",
    "#     plt.xlim(0, 1)\n",
    "#     plt.ylim(0, 7)\n",
    "#     plt.legend()\n",
    "#     plt.title(title)\n",
    "#     plt.show()\n",
    "\n",
    "# # Generate nonlinear data for regression testing.\n",
    "# def noisy_sine_data(n, s):\n",
    "#    rtn_x, rtn_y = [], []\n",
    "#    for i in range(n):\n",
    "#        x= np.random.random()\n",
    "#        y= 2.0*np.sin(2.0*np.pi*x)+np.random.normal(0.0, s) + 3.0\n",
    "#        rtn_x.append(x)\n",
    "#        rtn_y.append(y)\n",
    "#    return np.array(rtn_x).reshape(-1,1), np.array(rtn_y)\n",
    "\n",
    "# # Create training and test data\n",
    "# x_train, y_train = noisy_sine_data(n=500, s=0.5)\n",
    "# x_test = np.linspace(0, 1, 50).reshape(-1, 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth = 3\n",
    "\n",
    "# sk_model = DecisionTreeRegressor(max_depth = depth)\n",
    "# sk_model.fit(x_train, y_train)\n",
    "# sk_pred = sk_model.predict(x_test)\n",
    "\n",
    "# # Plot the training data and draw the estimated curve.\n",
    "# plot_prediction(x_train, y_train, x_test, sk_pred, 'DecisionTreeRegressor')\n",
    "\n",
    "# plt.figure(figsize=(12,7))\n",
    "# tree.plot_tree(sk_model)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some takeaways :\n",
    "\n",
    "* The shallower the tree, the more likely it is to be underfitting, and the deeper the tree the more likely it is to be overfitting. \n",
    "* As with classification pruning is necessary to prevent overfitting. CCP can be applied to the regression problem as well; instead of misclassification rate $R(T)$ we use MSE/SSR for regression.\n",
    "* Can also be applied for multiple features (multiple regression) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief background\n",
    "\n",
    "The first such algorithm was created in 1995 by Tin Kam Ho, while leading the Statistics and Learning Research Department at Bell Laboratories. Her work was then extended by Leo Breiman and Adele Cutler in 2001.\n",
    "\n",
    "more info here: https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregation (Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap aggregation, or \"bagging,\" involves repeatedly sampling data with replacement to create multiple datasets, training a decision tree on each. In a Random Forest, each tree is grown using these bootstrapped samples, and only a random subset of features is considered at each split, resulting in diverse trees; the model then combines predictions from all trees, often by majority voting for classification, to improve accuracy and reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data subsampling is a technique where multiple decision trees are trained on different subsets of rows and columns from the dataset without replacement. In Random Forests, each tree is built using a unique subset of data rows (row subsampling) and a random subset of features (column subsampling) at each split.\n",
    "\n",
    "more info here: https://youtu.be/sQ870aTKqiM?si=rRB0fqNIzyfih2De\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag (OOB) Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out-of-bag (OOB) score is an evaluation method used in Random Forests, where each tree is tested on the data points not included in its bootstrap sample (about one-third of the data). The OOB score is calculated by aggregating the predictions of these left-out points across all trees, providing an unbiased estimate of the model’s accuracy without needing a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming back to the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do some feature engineering to extract more features from a particular dataset. We do not have any idea currently if these features are statistically significant, but these are based on our discretion to what we believe will contain information that might not be evaluated in our current set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the titanic dataset into a dataframe\n",
    "# df = pd.read_csv('datasets/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the title of each passenger from the `Name` column. We do this by applying a function that looks through every element and splitting the element by its punctuation and then isolating the title. It also removes any leading or trailing whitespaces from the title. A lambda function is a small anonymous function that can take any number of arguments but only return one expression. We call it using the \n",
    ".apply() method.\n",
    "\n",
    "for more info on lambda functions: https://realpython.com/python-lambda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we create a new column called 'Title' that contains the title of the passengers\n",
    "# df['Title'] = df['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "# standardized_titles = {\n",
    "#     'Mr': 0, 'Miss': 0, 'Mrs': 0, 'Ms': 0,                              # Label 0 - Common Titles\n",
    "#     'Mlle': 1, 'Mme': 1,                                                # Label 1 - Young or Unmarried Ladies\n",
    "#     'Dr': 2, 'Rev': 2, 'Master': 2,                                     # Label 2 - Professional Titles\n",
    "#     'Capt': 3, 'Major': 3, 'Col': 3,                                    # Label 3 - Military/Official Titles\n",
    "#     'Sir': 4, 'Lady': 4, 'the Countess': 4, 'Don': 4, 'Jonkheer': 4     # Label 4 - Nobility Titles\n",
    "# }\n",
    "\n",
    "# # Map the normalized titles to the current titles in the 'Title' column\n",
    "# df['Title'] = df['Title'].map(standardized_titles)\n",
    "\n",
    "# # View value counts for the normalized titles\n",
    "# print(df['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another new feature, `Family_Size`. We can do this by adding the `Parch` and `Sibsp` features together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Family_size'] = df['Parch'] + df['SibSp'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the `PassengerID`, `Name`, `Ticket` and `Cabin` columns for now. Also we can see `Embarked` has 2 missing values so i'm using my discretion to add `N` to them. I'm also encoding `Sex` and `Embarked` features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "# df['Embarked'] = df['Embarked'].fillna('N')  \n",
    "# df['Sex'] = df['Sex'].factorize()[0]                \n",
    "# df['Embarked'] = df['Embarked'].factorize()[0]  \n",
    "# df.info()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data\n",
    "# y = np.array(df['Survived'])\n",
    "# x = np.array(df.drop('Survived', axis=1))\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# # Initially, missing values ​​in 'Age' are replaced with the average value.\n",
    "# AGE = 2  # column number of 'Age' feature\n",
    "\n",
    "# # the position of missing values. It is for later use.\n",
    "# i_train = np.where(np.isnan(x_train[:, AGE]))[0]  # training data\n",
    "# i_test = np.where(np.isnan(x_test[:, AGE]))[0]    # test data\n",
    "\n",
    "# # indices where y_train=0 and y_train=1\n",
    "# i_y0 = np.where(y_train == 0)[0]\n",
    "# i_y1 = np.where(y_train == 1)[0]\n",
    "\n",
    "# # the mean value of 'Age' where y_train=0, and the same where y_train=1.\n",
    "# y0_mean = np.nanmean(x_train[i_y0, AGE]) # where y_train=0\n",
    "# y1_mean = np.nanmean(x_train[i_y1, AGE]) # where y_train=1\n",
    "\n",
    "# # replace nan in 'Age' where y_train = 0 to y0_mean\n",
    "# x_train[i_y0, AGE] = np.nan_to_num(x_train[i_y0, AGE], nan=y0_mean)\n",
    "\n",
    "# # replace nan in 'Age' where y_train = 1 to y1_mean\n",
    "# x_train[i_y1, AGE] = np.nan_to_num(x_train[i_y1, AGE], nan=y1_mean)\n",
    "\n",
    "# # print('Before:\\n', x_train.round(2))\n",
    "# # print('y0_mean = {:.2f}, y1_mean = {:.2f}'.format(y0_mean, y1_mean))\n",
    "# plt.plot(x_train[i_train, AGE], 'bo')\n",
    "# plt.title('Initial values for the missing values')\n",
    "# plt.show()\n",
    "\n",
    "# print('Before:\\n', x_train.round(2))\n",
    "# print('y0_mean = {:.2f}, y1_mean = {:.2f}'.format(y0_mean, y1_mean))\n",
    "\n",
    "\n",
    "# # Create Proximity matrix\n",
    "# # normalize = 0: pm / n_tree\n",
    "# # normalize ≠ 0: Normalize columns to sum to 1\n",
    "# def proximity_matrix(model, x, normalize=0):\n",
    "#     n_tree = len(model.estimators_)\n",
    "    \n",
    "#     # Apply trees in the forest to X, return leaf indices.\n",
    "#     leaf = model.apply(x)  # shape = (x.shape[0], n_tree)\n",
    "    \n",
    "#     pm = np.zeros(shape=(x.shape[0], x.shape[0]))\n",
    "#     for i in range(n_tree):\n",
    "#         t = leaf[:, i]\n",
    "#         pm += np.equal.outer(t, t) * 1.\n",
    "\n",
    "#     np.fill_diagonal(pm, 0)    \n",
    "#     if normalize == 0:\n",
    "#         return pm / n_tree\n",
    "#     else:\n",
    "#         return pm / pm.sum(axis=0, keepdims=True)\n",
    "\n",
    "# n_estimators = 50\n",
    "# n_depth = 5\n",
    "\n",
    "# # Missing value imputation using the proximity matrix\n",
    "# for i in range(5):   # 5 iterations\n",
    "#     model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "#                                    max_depth=n_depth,\n",
    "#                                    oob_score=True)\n",
    "#     model.fit(x_train, y_train)\n",
    "    \n",
    "#     # Create proximity matrix\n",
    "#     pm = proximity_matrix(model, x_train, normalize=1)\n",
    "    \n",
    "#     # estimate the missing values of 'Age' using the proximity matrix\n",
    "#     x_age = x_train[:, AGE].copy()\n",
    "#     u_age = np.dot(x_age, pm)       # updated 'Age'\n",
    "#     x_train[i_train, AGE] = u_age[i_train]\n",
    "\n",
    "# print('\\nAfter:\\n', x_train.round(2))\n",
    "# plt.plot(x_train[i_train, AGE], 'ro')\n",
    "# plt.title('Estimated values for the missing values')\n",
    "# plt.show()\n",
    "\n",
    "# # Train a new model after imputing missing values of training data\n",
    "# model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "#                                max_depth=n_depth,\n",
    "#                                oob_score=True)\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# # Predict the test data. There are also missing values ​​in 'Age' in the test data.\n",
    "# #\n",
    "# # [2] Proximities\n",
    "# # When a test set is present, the proximities of each case in the test set \n",
    "# # with each case in the training set can also be computed.\n",
    "\n",
    "# # Initially, there is no target value y in the test data, so the missing values ​​\n",
    "# # are replaced with the mean value of the training data.\n",
    "# x_test[i_test, AGE] = x_train[:, AGE].mean()\n",
    "\n",
    "# x_data = np.vstack([x_train, x_test]) # combine training and test data\n",
    "# pm = proximity_matrix(model, x_data, normalize=1)\n",
    "# x_age = x_data[:, AGE].copy()         # 'Age' feature data\n",
    "# u_age = np.dot(x_age, pm)             # updated 'Age' feature\n",
    "\n",
    "# u_age = u_age[-x_test.shape[0]:]      # 'Age' of test data\n",
    "# u_test = x_data[-x_test.shape[0]:]    # test data\n",
    "# u_test[i_test, AGE] = u_age[i_test]   # update the missing values in test data\n",
    "\n",
    "# # predict\n",
    "# y_pred = model.predict(u_test)\n",
    "\n",
    "# print('\\nResults:')\n",
    "# print('Accuracy = {:.4f}'.format((y_pred == y_test).mean()))\n",
    "# print('Final OOB error rate = {:.4f}'.format(1 - model.oob_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
